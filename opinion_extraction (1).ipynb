{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7dcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73295e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"\"\"Compatibility of systems of linear constraints over the set of natural numbers.\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict inequations,\n",
    "and nonstrict inequations are considered. Upper bounds for components of a minimal set of\n",
    "solutions and algorithms of construction of minimal generating sets of solutions for all types\n",
    "of systems are given. These criteria and the corresponding algorithms for constructing a minimal\n",
    "supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\"\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "doc = nlp(text)\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model for English language\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text to analyze\n",
    "text = \"the food was decent, but the service was so fucking slow\"\n",
    "\n",
    "# Process the input text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate through noun chunks in the processed document\n",
    "for chunk in doc.noun_chunks:\n",
    "    # Print the text of the noun chunk, the text of the root token, the dependency label of the root token,\n",
    "    # and the text of the head token of the root token\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the spaCy model for English language\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the VADER SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Input text to analyze\n",
    "text = \"the food was decent, but the service was so fucking slow\"\n",
    "\n",
    "# Process the input text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# List to store extracted phrases and their sentiment scores\n",
    "phrases = []\n",
    "current_phrase = []\n",
    "\n",
    "# Iterate through tokens in the processed document\n",
    "for token in doc:\n",
    "    # Check if the token's dependency label is \"nsubj\" (nominal subject)\n",
    "    if token.dep_ == \"nsubj\":\n",
    "        # If there's an ongoing phrase, process it\n",
    "        if current_phrase:\n",
    "            # Combine the current phrase tokens into text\n",
    "            phrase_text = \" \".join(current_phrase)\n",
    "            # Calculate sentiment score for the phrase using VADER\n",
    "            sentiment_score = analyzer.polarity_scores(phrase_text)[\"compound\"]\n",
    "            # Append the phrase and its sentiment score to the list\n",
    "            phrases.append((phrase_text, sentiment_score))\n",
    "            # Reset the current phrase for the next iteration\n",
    "            current_phrase = []\n",
    "    # Append the token's text to the current phrase\n",
    "    current_phrase.append(token.text)\n",
    "\n",
    "# Append the last phrase (after the loop ends)\n",
    "if current_phrase:\n",
    "    phrase_text = \" \".join(current_phrase)\n",
    "    sentiment_score = analyzer.polarity_scores(phrase_text)[\"compound\"]\n",
    "    phrases.append((phrase_text, sentiment_score))\n",
    "\n",
    "# Print the extracted phrases and their sentiment scores\n",
    "for phrase, score in phrases:\n",
    "    print(\"Phrase:\", phrase)\n",
    "    print(\"Sentiment Score:\", score)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the spaCy model for English language\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text to analyze\n",
    "text = \"Dont really care. Its gonna happen no matter what we do. Go ahead be a vegan and try to stop cow farts. Not gonna matter. I care more about jobs and america first than fking polar bears. We have pictures of them. Good enough\"\n",
    "\n",
    "# Process the input text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# List to store extracted phrases and their sentiment scores\n",
    "phrases = []\n",
    "current_phrase = []\n",
    "\n",
    "# Iterate through tokens in the processed document\n",
    "for token in doc:\n",
    "    # Check if the token's dependency label is \"nsubj\" (nominal subject)\n",
    "    if token.dep_ == \"nsubj\":\n",
    "        # If there's an ongoing phrase, process it\n",
    "        if current_phrase:\n",
    "            # Combine the current phrase tokens into text\n",
    "            phrase_text = \" \".join(current_phrase)\n",
    "            # Calculate sentiment polarity for the phrase using TextBlob\n",
    "            sentiment = TextBlob(phrase_text).sentiment.polarity\n",
    "            # Append the phrase and its sentiment polarity to the list\n",
    "            phrases.append((phrase_text, sentiment))\n",
    "            # Reset the current phrase for the next iteration\n",
    "            current_phrase = []\n",
    "    # Append the token's text to the current phrase\n",
    "    current_phrase.append(token.text)\n",
    "\n",
    "# Append the last phrase (after the loop ends)\n",
    "if current_phrase:\n",
    "    phrase_text = \" \".join(current_phrase)\n",
    "    sentiment = TextBlob(phrase_text).sentiment.polarity\n",
    "    phrases.append((phrase_text, sentiment))\n",
    "\n",
    "# Print the extracted phrases and their sentiment scores\n",
    "for phrase, score in phrases:\n",
    "    print(\"Phrase:\", phrase)\n",
    "    print(\"Sentiment Score:\", score)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('community a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182dc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('community b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize empty lists to store extracted phrases, sentiments, and scores\n",
    "phrases = []\n",
    "sentiments = []\n",
    "scores = []\n",
    "\n",
    "# Process each comment in the DataFrame\n",
    "for idx, row in df.iterrows():\n",
    "    comment = row['body']\n",
    "    doc = nlp(comment)\n",
    "    current_phrase = []\n",
    "\n",
    "    # Iterate through tokens in the processed document\n",
    "    for token in doc:\n",
    "        # Check if the token's dependency label is \"nsubj\" (nominal subject)\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            # If there's an ongoing phrase, process it\n",
    "            if current_phrase:\n",
    "                # Combine the current phrase tokens into text\n",
    "                phrase_text = \" \".join(current_phrase)\n",
    "                # Calculate sentiment polarity for the phrase using TextBlob\n",
    "                sentiment = TextBlob(phrase_text).sentiment.polarity\n",
    "                # Append the extracted phrase, sentiment, and comment score to lists\n",
    "                phrases.append(phrase_text)\n",
    "                sentiments.append(sentiment)\n",
    "                scores.append(row['score'])\n",
    "                # Reset the current phrase for the next iteration\n",
    "                current_phrase = []\n",
    "        # Append the token's text to the current phrase\n",
    "        current_phrase.append(token.text)\n",
    "\n",
    "    # Append the last phrase (after the loop ends)\n",
    "    if current_phrase:\n",
    "        phrase_text = \" \".join(current_phrase)\n",
    "        sentiment = TextBlob(phrase_text).sentiment.polarity\n",
    "        phrases.append(phrase_text)\n",
    "        sentiments.append(sentiment)\n",
    "        scores.append(row['score'])\n",
    "\n",
    "# Create a new DataFrame with extracted phrases, sentiments, and scores\n",
    "df_phrases = pd.DataFrame({\n",
    "    'Phrase': phrases,\n",
    "    'Sentiment': sentiments,\n",
    "    'Score': scores\n",
    "    'Topic': topic\n",
    "})\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "df_phrases.to_csv('phrases_sentiments_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34387e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize empty lists\n",
    "phrases = []\n",
    "sentiments = []\n",
    "scores = []\n",
    "\n",
    "# Process each comment in the DataFrame\n",
    "for idx, row in df1.iterrows():\n",
    "    comment = row['body']\n",
    "    doc = nlp(comment)\n",
    "    current_phrase = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            if current_phrase:\n",
    "                phrase_text = \" \".join(current_phrase)\n",
    "                sentiment = TextBlob(phrase_text).sentiment.polarity\n",
    "                phrases.append(phrase_text)\n",
    "                sentiments.append(sentiment)\n",
    "                scores.append(row['score'])\n",
    "                current_phrase = []\n",
    "        current_phrase.append(token.text)\n",
    "\n",
    "    # Append the last phrase\n",
    "    if current_phrase:\n",
    "        phrase_text = \" \".join(current_phrase)\n",
    "        sentiment = TextBlob(phrase_text).sentiment.polarity\n",
    "        phrases.append(phrase_text)\n",
    "        sentiments.append(sentiment)\n",
    "        scores.append(row['score'])\n",
    "\n",
    "# Create a new DataFrame with phrases, sentiments, and scores\n",
    "df_phrases_1 = pd.DataFrame({\n",
    "    'Phrase': phrases,\n",
    "    'Sentiment': sentiments,\n",
    "    'Score': scores\n",
    "    'Topic': topic\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_phrases_1.to_csv('phrases_sentiments_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c675321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phrases['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the averaged perceptron tagger and WordNet corpus from NLTK\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Create an instance of the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = df_phrases['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores_1 = df_phrases_1['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ba08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "plt.hist(sentiment_scores, bins=20, edgecolor='black')\n",
    "plt.title('Sentiment Distribution of Reddit Comments')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "plt.hist(sentiment_scores_1, bins=20, edgecolor='black')\n",
    "plt.title('Sentiment Distribution of Reddit Comments')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = df_phrases['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3280b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_1 = df_phrases_1['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store topic sentiments\n",
    "# The DataFrame will have two columns: 'Topic' to store the topics and 'Sentiment' to store corresponding sentiment scores\n",
    "topic_sentiments = pd.DataFrame({'Topic': topics, 'Sentiment': df_phrases['Sentiment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store topic sentiments\n",
    "# The DataFrame will have two columns: 'Topic' to store the topics and 'Sentiment' to store corresponding sentiment scores\n",
    "topic_sentiments_1 = pd.DataFrame({'Topic': topics_1, 'Sentiment': df_phrases_1['Sentiment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sentiments = topic_sentiments.groupby('Topic')['Sentiment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sentiments_1 = topic_sentiments_1.groupby('Topic')['Sentiment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d52ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentiments = average_sentiments.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentiments_1 = average_sentiments_1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16403665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create DataFrames from the Series\n",
    "df_phrases = sorted_sentiments.reset_index()\n",
    "df_phrases_1 = sorted_sentiments_1.reset_index()\n",
    "\n",
    "# Merge the two DataFrames based on the 'Topic' column\n",
    "merged_df = pd.merge(df_phrases_1, df_phrases, on='Topic', suffixes=('_a', '_b'))\n",
    "\n",
    "# Calculate the absolute difference in sentiment between Sentiment A and Sentiment B\n",
    "merged_df['Sentiment_diff'] = abs(merged_df['Sentiment_a'] - merged_df['Sentiment_b'])\n",
    "\n",
    "# Get the topics with the greatest sentiment differences\n",
    "top_diff_topics = merged_df.nlargest(10, 'Sentiment_diff')['Topic']\n",
    "\n",
    "# Filter the merged DataFrame to include only the top difference topics\n",
    "filtered_df = merged_df[merged_df['Topic'].isin(top_diff_topics)]\n",
    "\n",
    "# Manual labels for the x-axis\n",
    "x_labels = ['label 1','label 2','label 3']\n",
    "\n",
    "# Plot the filtered topics and their sentiments\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_df['Topic'], filtered_df['Sentiment_a'], label='Sentiment A')\n",
    "plt.bar(filtered_df['Topic'], filtered_df['Sentiment_b'], label='Sentiment B', alpha=0.7)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Comparison of Sentiments for Topics with Greatest Differences')\n",
    "plt.xticks(filtered_df['Topic'], x_labels, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topics with the greatest sentiment differences\n",
    "top_diff_topics = merged_df.nlargest(10, 'Sentiment_diff')['Topic']\n",
    "\n",
    "# Filter the merged DataFrame to include only the top difference topics\n",
    "filtered_df = merged_df[merged_df['Topic'].isin(top_diff_topics)]\n",
    "\n",
    "# Manual labels for the x-axis\n",
    "x_labels = ['label 1','label 2','label 3']\n",
    "\n",
    "# Plot the filtered topics and their sentiments\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_df['Topic'], filtered_df['Sentiment_a'], label='Sentiment A')\n",
    "plt.bar(filtered_df['Topic'], filtered_df['Sentiment_b'], label='Sentiment B', alpha=0.7)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Comparison of Sentiments for Topics with Greatest Differences')\n",
    "plt.xticks(filtered_df['Topic'], x_labels, rotation=45, ha='right')\n",
    "plt.yticks()  \n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure.png', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
