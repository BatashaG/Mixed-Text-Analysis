{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e261a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403811dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87173a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = pd.read_csv('edge_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list.rename(columns={'author': 'Source'}, inplace=True)\n",
    "edge_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges to the graph from the DataFrame\n",
    "for _, row in edge_list.iterrows():\n",
    "    source = row['Source']\n",
    "    target = row['Target']\n",
    "    G.add_edge(source, target)\n",
    "\n",
    "# Calculate degree centrality for each node\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Select the top influential nodes based on degree centrality\n",
    "num_nodes_to_label = 20  # Adjust the number of nodes to label\n",
    "sorted_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
    "selected_nodes = sorted_nodes[:num_nodes_to_label]\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 10))  # Adjust the figure size if needed\n",
    "pos = nx.spring_layout(G)  # Choose a layout algorithm for the graph\n",
    "\n",
    "# Specify node and edge colors, sizes, and styles\n",
    "node_size = 30  # Adjust the node size\n",
    "font_size = 6  # Adjust the font size\n",
    "node_color = 'lightblue'\n",
    "edge_color = 'black'\n",
    "\n",
    "nx.draw_networkx(G, pos=pos, with_labels=False, node_color=node_color, edge_color=edge_color,\n",
    "                 node_size=node_size, alpha=0.7)\n",
    "\n",
    "# Create a dictionary of labels for the selected nodes\n",
    "labels = {node: str(node) for node in selected_nodes if node in G.nodes}\n",
    "\n",
    "nx.draw_networkx_labels(G, pos=pos, labels=labels, font_size=font_size, font_color='black')\n",
    "\n",
    "plt.axis('off')  # Remove the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa62de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0584eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community\n",
    "\n",
    "# Calculate degree centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Calculate clustering coefficient\n",
    "clustering_coefficient = nx.clustering(G)\n",
    "\n",
    "# Perform community detection using Louvain algorithm\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Assign community labels to nodes\n",
    "community_assignment = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        community_assignment[node] = i\n",
    "\n",
    "c = 0  # Initialize 'c' with 0\n",
    "for node in G.nodes:\n",
    "    if clustering_coefficient[node] != 0:\n",
    "        print(f\"Node: {node}\")\n",
    "        print(f\"Degree Centrality: {degree_centrality[node]}\")\n",
    "        print(f\"Betweenness Centrality: {betweenness_centrality[node]}\")\n",
    "        print(f\"Clustering Coefficient: {clustering_coefficient[node]}\")\n",
    "        print(f\"Community: {community_assignment[node]}\")\n",
    "        print()\n",
    "        c += 1\n",
    "\n",
    "print(f\"Number of nodes with non-zero clustering coefficient: {c}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106af16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Calculate clustering coefficient\n",
    "clustering_coefficient = nx.clustering(G)\n",
    "\n",
    "# Perform community detection using Louvain algorithm\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Assign community labels to nodes\n",
    "community_assignment = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        community_assignment[node] = i\n",
    "\n",
    "\n",
    "for node in G.nodes:\n",
    "    print(f\"Node: {node}\")\n",
    "    print(f\"Degree Centrality: {degree_centrality[node]}\")\n",
    "    print(f\"Betweenness Centrality: {betweenness_centrality[node]}\")\n",
    "    print(f\"Clustering Coefficient: {clustering_coefficient[node]}\")\n",
    "    print(f\"Community: {community_assignment[node]}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ed6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate degree centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Calculate clustering coefficient\n",
    "clustering_coefficient = nx.clustering(G)\n",
    "\n",
    "# Perform community detection using Louvain algorithm\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Assign community labels to nodes\n",
    "community_assignment = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        community_assignment[node] = i\n",
    "\n",
    "# Create an empty DataFrame to store the centrality and community assignment values\n",
    "centrality_df = pd.DataFrame(columns=['Node', 'Degree Centrality', 'Betweenness Centrality', 'Clustering Coefficient', 'Community'])\n",
    "\n",
    "# Populate the DataFrame with centrality and community assignment values\n",
    "for node in G.nodes:\n",
    "    centrality_df = centrality_df.append({\n",
    "        'Node': node,\n",
    "        'Degree Centrality': degree_centrality[node],\n",
    "        'Betweenness Centrality': betweenness_centrality[node],\n",
    "        'Clustering Coefficient': clustering_coefficient[node],\n",
    "        'Community': community_assignment[node]\n",
    "    }, ignore_index=True)\n",
    "\n",
    "\n",
    "# Merge the 'edge_list' DataFrame with the 'centrality_df' DataFrame based on the common 'Source' column\n",
    "\n",
    "centrality_and_community_with_edge_data = centrality_df.merge(edge_list, left_on='Node', right_on='Source', how='left')\n",
    "\n",
    "# Print the DataFrame with additional data\n",
    "print(centrality_and_community_with_edge_data)\n",
    "\n",
    "# Save the DataFrame with additional data to a CSV file\n",
    "centrality_and_community_with_edge_data.to_csv('centrality_and_community_with_edge_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a64373",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_df=centrality_and_community_with_edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histogram of clustering coefficients\n",
    "plt.hist(centrality_df['Clustering Coefficient'], bins=20, edgecolor='black')\n",
    "plt.xlabel('Clustering Coefficient')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.title('Distribution of Clustering Coefficients')\n",
    "plt.savefig('figure.png',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = centrality_df['Clustering Coefficient']\n",
    "y = centrality_df['Degree Centrality']\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.scatter(x, y, alpha=0.5)\n",
    "plt.xlabel('Degree Centrality')\n",
    "plt.ylabel('Clustering Coefficient')\n",
    "plt.title('Scatter Plot: Degree Centrality vs. Clustering Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a list of nodes and their betweenness centrality values\n",
    "nodes = centrality_df['Node']\n",
    "values = centrality_df['Betweenness Centrality']\n",
    "# Create a scatter plot where the size of each node represents its betweenness centrality\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(nodes, values, s=[v * 5000 for v in values], alpha=0.7)\n",
    "plt.xlabel('Nodes')\n",
    "plt.ylabel('Betweenness Centrality')\n",
    "plt.title('Betweenness Centrality Visualization (technology)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure.png',dpi=300)\n",
    "plt.show()\n",
    "#put topics on the node axis- topicsthat act as bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = centrality_df[centrality_df['Betweenness Centrality'] > 0.12]\n",
    "\n",
    "# Print the 'Node' and 'Topic' columns for the filtered rows\n",
    "for index, row in filtered_df.iterrows():\n",
    "    print(f\"Node: {row['Node']}, Topic: {row['Topic']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff41916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community\n",
    "\n",
    "# Perform community detection using Louvain algorithm\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Create a dictionary to store community node counts\n",
    "community_node_counts = {}\n",
    "\n",
    "# Count the nodes in each community\n",
    "for i, comm in enumerate(communities):\n",
    "    community_node_counts[i] = len(comm)\n",
    "\n",
    "# Print the community node counts\n",
    "for community_id, node_count in community_node_counts.items():\n",
    "    print(f\"Community {community_id}: {node_count} nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from networkx.algorithms import community\n",
    "\n",
    "# Perform community detection using Louvain algorithm\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Filter the original dataframe for Community 0\n",
    "community_nodes = [node for node, comm_id in community_assignment.items() if comm_id == 0]\n",
    "community_df = edge_list[edge_list['Source'].isin(community_nodes)].copy()\n",
    "\n",
    "# Store the dataframe for Community 0 in a variable\n",
    "community_0_dataframe = community_df.copy()\n",
    "\n",
    "# Print the dataframe for Community 0\n",
    "community_0_dataframe\n",
    "community_df.to_csv('communoty 0.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1334f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community\n",
    "\n",
    "\n",
    "# Create a graph from the edge list\n",
    "G = nx.from_pandas_edgelist(edge_list, 'Source', 'Target')\n",
    "\n",
    "# Perform community detection using Louvain algorithm\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Assign community labels to nodes\n",
    "community_assignment = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        community_assignment[node] = i\n",
    "\n",
    "# Analyze posts with commenters dispersed across communities\n",
    "dispersed_posts = []\n",
    "for post_title, group in edge_list.groupby('post_title'):\n",
    "    commenters = set(group['Source'])\n",
    "    community_ids = set(community_assignment.get(commenter) for commenter in commenters)\n",
    "    if len(community_ids) > 1:\n",
    "        dispersity = len(community_ids) / len(commenters)  # Calculate dispersity as the ratio of unique communities to total commenters\n",
    "        subreddit = group['subreddit'].iloc[0]  # Get the subreddit for the post\n",
    "        dispersed_posts.append((post_title, dispersity, subreddit))\n",
    "\n",
    "# Sort the dispersed posts by dispersity in descending order\n",
    "dispersed_posts_sorted = sorted(dispersed_posts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the dispersed posts and their dispersity in order of dispersity\n",
    "print(\"Posts with commenters dispersed across communities (in order of dispersity):\")\n",
    "for post_title, dispersity, subreddit in dispersed_posts_sorted:\n",
    "    print(f\"Post: {post_title}, Dispersity: {dispersity}, Subreddit: {subreddit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a75d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform community detection using Louvain algorithm\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Assign community labels to nodes\n",
    "community_assignment = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        community_assignment[node] = i\n",
    "\n",
    "# Analyze posts with commenters dispersed across communities\n",
    "dispersed_posts = set()\n",
    "for post_title, group in edge_list.groupby('post_title'):\n",
    "    commenters = set(group['Source'])\n",
    "    community_ids = set(community_assignment.get(commenter) for commenter in commenters)\n",
    "    if len(community_ids) > 1:\n",
    "        dispersed_posts.add(post_title)\n",
    "\n",
    "# Print the number of post titles and the number of post titles with dispersity\n",
    "num_post_titles = len(edge_list['post_title'].unique())\n",
    "num_post_titles_with_dispersity = len(dispersed_posts)\n",
    "\n",
    "print(\"Number of post titles:\", num_post_titles)\n",
    "print(\"Number of post titles with dispersity:\", num_post_titles_with_dispersity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform community detection using Louvain algorithm\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Assign community labels to nodes\n",
    "community_assignment = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        community_assignment[node] = i\n",
    "\n",
    "# Analyze posts with commenters dispersed across communities\n",
    "dispersed_posts = []\n",
    "for post_title, group in edge_list.groupby('post_title'):\n",
    "    commenters = set(group['Source'])\n",
    "    community_ids = set(community_assignment.get(commenter) for commenter in commenters)\n",
    "    if len(community_ids) > 1:\n",
    "        dispersity = len(community_ids) / len(commenters)  # Calculate dispersity as the ratio of unique communities to total commenters\n",
    "        subreddit = group['subreddit'].iloc[0]  # Get the subreddit for the post\n",
    "        dispersed_posts.append((post_title, dispersity, subreddit))\n",
    "\n",
    "# Sort dispersed posts by dispersity in descending order\n",
    "dispersed_posts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract post titles and dispersities for visualization\n",
    "post_titles = [post[0] for post in dispersed_posts]\n",
    "dispersities = [post[1] for post in dispersed_posts]\n",
    "\n",
    "# Create a bar chart to visualize dispersities\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(dispersed_posts)), dispersities)\n",
    "plt.xlabel('Post')\n",
    "plt.ylabel('Dispersity')\n",
    "plt.title('Dispersity of Commenters Across Communities')\n",
    "plt.xticks(range(len(dispersed_posts)), post_titles, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ffc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a subgraph of edges connecting commenters from different communities\n",
    "edges = [(source, target) for source, target in G.edges() if community_assignment[source] != community_assignment[target]]\n",
    "subgraph = G.edge_subgraph(edges)\n",
    "\n",
    "# Draw the network graph\n",
    "pos = nx.spring_layout(subgraph)\n",
    "nx.draw_networkx_nodes(subgraph, pos, node_color='lightblue', node_size=200)\n",
    "nx.draw_networkx_edges(subgraph, pos, edge_color='gray')\n",
    "nx.draw_networkx_labels(subgraph, pos, font_size=8)\n",
    "plt.title('Dispersity of Commenters across Communities')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aaee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Create nodes and links for the Sankey diagram\n",
    "nodes = list(set(subreddits))\n",
    "node_indices = {node: index for index, node in enumerate(nodes)}\n",
    "source_indices = [node_indices[subreddit] for _, _, subreddit in dispersed_posts]\n",
    "target_indices = [(index + 1) % len(nodes) for index in source_indices]\n",
    "link_values = [1] * len(dispersed_posts)\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(label=nodes),\n",
    "    link=dict(source=source_indices, target=target_indices, value=link_values)\n",
    ")])\n",
    "fig.update_layout(title_text='Dispersity of Commenters across Subreddits')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Extract dispersity and subreddit information\n",
    "dispersity_values = [dispersity for _, dispersity, _ in dispersed_posts]\n",
    "subreddits = list(set(subreddits))\n",
    "num_subreddits = len(subreddits)\n",
    "\n",
    "# Create an empty matrix to store the connections\n",
    "matrix = [[0] * num_subreddits for _ in range(num_subreddits)]\n",
    "\n",
    "# Fill the matrix with the dispersity values\n",
    "for post_title, dispersity, subreddit in dispersed_posts:\n",
    "    source_index = subreddits.index(subreddit)\n",
    "    for target_index in range(num_subreddits):\n",
    "        if target_index != source_index:\n",
    "            matrix[source_index][target_index] += 1\n",
    "\n",
    "# Create the chord diagram\n",
    "fig = go.Figure(data=[go.Chord(\n",
    "    matrix=matrix,\n",
    "    labels=subreddits,\n",
    "    colorscale='Viridis',\n",
    "    hovertext=subreddits,\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Dispersity of Commenters across Subreddits',\n",
    "    font_size=12,\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
